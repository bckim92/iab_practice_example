{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [OpenCV-Python Tutorial] Classification\n",
    "\n",
    "In this notebook, we will learn how to perform Object Classification using Visual Bag of Words(VBoW), spatial histogram feature, and SVM classifier.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python==3.3.0.10\n",
    "!pip install opencv-contrib-python==3.3.0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import tarfile\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset\n",
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp\" -O 101_ObjectCategories.tar.gz && rm -rf /tmp/cookies.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caltech_url = 'http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz'\n",
    "caltech_filename = '101_ObjectCategories.tar.gz'\n",
    "caltech_dir = '101_ObjectCategories/'\n",
    "\n",
    "numTrain = 15\n",
    "numTest = 15\n",
    "numClasses = 102\n",
    "numWords = 600\n",
    "\n",
    "vocabPath = 'vocab.pkl'\n",
    "svmPath = os.path.join(caltech_dir, 'svm_data.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Caltech-101 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(caltech_dir) or not os.path.exists(os.path.join(caltech_dir, 'airplanes')):\n",
    "    print('Extracting Caltech-101')\n",
    "    with tarfile.open(caltech_filename) as tar:\n",
    "        tar.extractall()\n",
    "\n",
    "print('Caltech-101 dataset checked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense SIFT(PHOW) and Visual Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 15 train/test images for each class\n",
    "print('Select 15 train/test images for each class')\n",
    "classes = next(os.walk(caltech_dir))[1]\n",
    "train_image_ll = []\n",
    "test_image_ll = []\n",
    "for c in classes:\n",
    "    class_dir = os.path.join(caltech_dir, c, '*.jpg')\n",
    "    ims = glob(class_dir)\n",
    "    ims.sort()\n",
    "    train_image_ll.append([f for f in ims[:numTrain]])\n",
    "    test_image_ll.append([f for f in ims[numTrain:numTrain+numTest]])\n",
    "print('done')\n",
    "\n",
    "# Show 4 random images\n",
    "plt.figure(figsize=(16,4))\n",
    "for i, idx in enumerate(random.sample(range(len(classes)), 4)):\n",
    "    fpath = random.sample(train_image_ll[idx], 1)[0]\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.imshow(cv2.cvtColor(cv2.imread(fpath), cv2.COLOR_BGR2RGB))\n",
    "    plt.xlabel(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SIFT helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# Resize a image if it's too large\n",
    "def standarizeImage(img):\n",
    "    rows, cols = img.shape\n",
    "    if cols > 480:\n",
    "        img = cv2.resize(img, (480, int(rows*480/cols)))\n",
    "    return img\n",
    "\n",
    "# Detect and extract SIFT from a single image\n",
    "def SIFT(img):\n",
    "    kp, des = sift.detectAndCompute(img, None)\n",
    "    return kp, des\n",
    "\n",
    "# Load an image + Resize if large + Extract SIFT\n",
    "def SIFT2(img_fpath):\n",
    "    img = cv2.imread(img_fpath, cv2.IMREAD_GRAYSCALE)\n",
    "    img = standarizeImage(img)\n",
    "    return SIFT(img)\n",
    "\n",
    "# Dense SIFT(Extract SIFT descriptor in grid points over an image)\n",
    "def denseSIFT(img, step = 10, size = 7):\n",
    "    rows, cols = img.shape[:2]\n",
    "    kp = []\n",
    "    for x in range(step,cols,step):\n",
    "        for y in range(step,rows,step):\n",
    "            kp.append(cv2.KeyPoint(x, y, size))\n",
    "    kp, des = sift.compute(img, kp)\n",
    "    return kp, des\n",
    "\n",
    "# Load an image + Resize if large + Dense SIFT\n",
    "def denseSIFT2(img_fpath, step = 10):\n",
    "    img = cv2.imread(img_fpath, cv2.IMREAD_GRAYSCALE)\n",
    "    img = standarizeImage(img)\n",
    "    return denseSIFT(img, step)\n",
    "\n",
    "# Dense SIFT\n",
    "tmp_im = cv2.cvtColor(cv2.imread(train_image_ll[0][0]), cv2.COLOR_BGR2RGB)\n",
    "#kp, des = SIFT(tmp_im)\n",
    "kp, des = denseSIFT(tmp_im)\n",
    "im_sift = cv2.drawKeypoints(tmp_im, kp, None)\n",
    "plt.imshow(cv2.cvtColor(im_sift, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visual Words(Train Vocabulary)"
   ]
  },
   {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-extracted vocab.pkl\n",
    "!wget https://raw.githubusercontent.com/bckim92/iab_practice_example/master/vocab.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train vocabulary\n",
    "if not os.path.exists(vocabPath):\n",
    "    # Get PHOW features from 30 random training image to build a dictionary\n",
    "    print(\"Extracting PHOW features some training images...\")\n",
    "    PHOW_descrs = []\n",
    "    temp = [item for sublist in train_image_ll for item in sublist]\n",
    "    temp = random.sample(temp, 30)\n",
    "    for fpath in temp:\n",
    "        _, des = denseSIFT2(fpath)\n",
    "        PHOW_descrs.append(des)\n",
    "    PHOW_descrs = np.concatenate(PHOW_descrs, axis=0)\n",
    "    print(\"Total {} PHOW features\".format(PHOW_descrs.shape[0]))\n",
    "    \n",
    "    # Quantize the descriptors to get the visual words\n",
    "    print(\"Running K-means clustering ({} -> {})...\".format(PHOW_descrs.shape[0], numWords))\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 500, 1.0)\n",
    "    attempts = 10\n",
    "    flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "    start_time = time.time()\n",
    "    retval, bestLabels, vocab = cv2.kmeans(PHOW_descrs, numWords, None, criteria, attempts, flags)\n",
    "    print('Elapsed time: {:6}s'.format(time.time() - start_time))\n",
    "    \n",
    "    print('Saving...') \n",
    "    with open(vocabPath, 'wb') as fd:\n",
    "        pickle.dump(vocab, fd)\n",
    "else:\n",
    "    print(\"Load the trained visual words...\")\n",
    "    with open(vocabPath, 'rb') as fd:\n",
    "        vocab = pickle.load(fd)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "myCounter = collections.Counter(bestLabels.flatten())\n",
    "plt.plot(sorted(myCounter.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = cv2.BFMatcher()\n",
    "\n",
    "def getImageDescriptor(img, step=10, size=7):\n",
    "    img = standarizeImage(img)\n",
    "    cols, rows = img.shape[:2]\n",
    "    \n",
    "    # Extracting denseSIFT and BoW\n",
    "    kp = []\n",
    "    for x in range(step,cols,step):\n",
    "        for y in range(step,rows,step):\n",
    "            kp.append(cv2.KeyPoint(x, y, size))\n",
    "    kp, des = sift.compute(img, kp)\n",
    "    matches = bf.knnMatch(des, vocab, k=1)\n",
    "    words = [m[0].trainIdx for m in matches]\n",
    "    \n",
    "    # Spatial Binning - 2x2\n",
    "    binX = 2; binY = 2;\n",
    "    temp = np.zeros((binX, binY, numWords), dtype=np.float32)\n",
    "    for k, w in zip(kp, words):\n",
    "        i = int((k.pt[0]) * binX / cols)\n",
    "        j = int((k.pt[1]) * binY / rows)\n",
    "        temp[i, j, w] += 1\n",
    "    for i in range(binX):\n",
    "        for j in range(binY):\n",
    "            temp[i, j, :] /= np.sum(temp[i, j, :])\n",
    "    temp = temp.flatten()\n",
    "    hist = temp\n",
    "    \n",
    "    # Spatial Binning - 4x4\n",
    "    ################################\n",
    "    # TODO : your code here\n",
    "    temp = []\n",
    "    ################################\n",
    "    hist = np.concatenate((hist, temp), axis=0)\n",
    "    \n",
    "    \n",
    "    hist /= np.sum(hist)\n",
    "    return hist    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Genarate spatial histogram for Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract spartial histogram for all training images\n",
    "print('Extract spartial histogram for all training images')\n",
    "BoW_train_ll = []\n",
    "for c, image_list in zip(classes, train_image_ll):\n",
    "    print(c, end=' '), \n",
    "    temp = []\n",
    "    for fpath in image_list:\n",
    "        img = cv2.imread(fpath, cv2.IMREAD_GRAYSCALE)\n",
    "        temp.append(getImageDescriptor(img))\n",
    "    BoW_train_ll.append(temp)\n",
    "\n",
    "print('\\ndone!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting training data(BoW, label) for SVM\n",
    "print('Setting training data(BoW, label) for SVM')\n",
    "train_bow = []\n",
    "train_labels = []\n",
    "numSpartialHist = len(BoW_train_ll[0][0])\n",
    "for i, BoW_list in enumerate(BoW_train_ll):\n",
    "    for bow in BoW_list:\n",
    "        # Hellinger's kernel for each training instance(bow)\n",
    "        temp = np.sqrt(bow)\n",
    "        temp -= np.average(temp)\n",
    "        if np.std(temp) != 0:\n",
    "            temp /= np.std(temp)\n",
    "        train_bow.append([temp])\n",
    "    train_labels.extend([i] * len(BoW_list))\n",
    "train_bow = np.concatenate(train_bow, axis=0).astype(np.float32)\n",
    "train_labels = np.array(train_labels, dtype=np.int).reshape((numTrain*numClasses,1))\n",
    "# => train_bow: (1530, 12000), train_labels: (1530,1)\n",
    "\n",
    "print(\"train_bow: \", str(train_bow.shape))\n",
    "print(\"train_labels: \", str(train_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train SVM\n",
    "print('Training SVM...')\n",
    "svm = cv2.ml.SVM_create()\n",
    "svm.setType(cv2.ml.SVM_C_SVC)\n",
    "svm.setKernel(cv2.ml.SVM_LINEAR)\n",
    "svm.setC(0.01)\n",
    "svm.setTermCriteria((cv2.TERM_CRITERIA_COUNT, 10, 1.0))\n",
    "\n",
    "start_time = time.time()\n",
    "# svm.train(SVM_train_data)\n",
    "svm.train(train_bow, cv2.ml.ROW_SAMPLE, train_labels)\n",
    "print('Elapsed time: {:6}s'.format(time.time() - start_time))\n",
    "\n",
    "# print 'Saving SVM...'\n",
    "# svm.save(svmPath)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = svm.predict(train_bow)[1]\n",
    "print('Training Accuracy: %.6f' % np.average(train_preds == train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test trained SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract spartial histogram for all test images\n",
    "################################\n",
    "# TODO : your code here\n",
    "print('Extract spartial histogram for all test images\\n')\n",
    "BoW_test_ll = []\n",
    "\n",
    "print('\\ndone!')\n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting test data(BoW, label) for SVM\n",
    "print('Setting test data(BoW, label) for SVM')\n",
    "################################\n",
    "# TODO : your code here\n",
    "test_bow = []\n",
    "test_labels = []\n",
    "numSpartialHist = len(BoW_test_ll[0][0])\n",
    "\n",
    "# TODO : inference code here\n",
    "\n",
    "test_bow = None\n",
    "test_labels = None\n",
    "print(test_bow.shape)\n",
    "print(test_labels.shape)\n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = svm.predict(test_bow)[1]\n",
    "print('Test Accuracy: {:6}'.format(np.average(test_preds == test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sample Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 8 random images\n",
    "plt.figure(figsize=(16,10))\n",
    "for i, r_c in enumerate(random.sample(range(len(classes)), 8)):\n",
    "    # Random sample image from given class\n",
    "    r_i = random.sample(range(numTest), 1)\n",
    "    fpath = train_image_ll[r_c][r_i[0]]\n",
    "    img = cv2.imread(fpath, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Get predicted label\n",
    "    test_res = int(test_preds[r_c * numTest + r_i[0]][0])\n",
    "    \n",
    "    plt.subplot(2, 4, i+1)\n",
    "    plt.imshow(cv2.cvtColor(cv2.imread(fpath), cv2.COLOR_BGR2RGB))\n",
    "    plt.xlabel(\"GT: {} / Predict: {}\".format(classes[r_c], classes[test_res]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Practice: Classify your own image\n",
    "\n",
    "Choose and classify your own test image using our classification.\n",
    "\n",
    "- Place your image in the path where this notebook can access\n",
    "- Load an image as grayscale\n",
    "- Extract Dense SIFT BoW with `getImageDescriptor()`\n",
    "- Normalize the bow with Hellinger's kernel\n",
    "- Predict using the SVM classifier we have trained so far(`svm` instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/bckim92/iab_practice_example/master/images/dragonfly.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fpath = './dragonfly.jpg'  # Path to your own image\n",
    "img = cv2.imread(img_fpath, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#================ YOUR CODE HERE ===================\n",
    "\n",
    "\n",
    "prediction = 0  # Predicted class of your image\n",
    "#===================================================\n",
    "\n",
    "plt.imshow(cv2.cvtColor(cv2.imread(img_fpath), cv2.COLOR_BGR2RGB))\n",
    "plt.xlabel(\"Prediction: {}\".format(classes[prediction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
